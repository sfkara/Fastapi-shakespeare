{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq5DELFA56N6",
        "outputId": "015a74ed-6738-4a81-d7da-c8d3f91dc0f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "# Load the dataset\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "response = urllib.request.urlopen(url)\n",
        "data = response.read()\n",
        "text = data.decode('utf-8')"
      ],
      "metadata": {
        "id": "zN5YYUP_6IY9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Encode the text\n",
        "encoded_text = tokenizer.encode(text)\n",
        "\n",
        "# Create input sequences\n",
        "input_sequences = []\n",
        "for i in range(0, len(encoded_text) - 1024, 1024):\n",
        "    input_sequences.append(encoded_text[i:i+1024])\n",
        "\n",
        "# Pad the sequences\n",
        "max_length = max([len(seq) for seq in input_sequences])\n",
        "padded_sequences = []\n",
        "for seq in input_sequences:\n",
        "    padded_sequences.append(seq + [tokenizer.pad_token_id] * (max_length - len(seq)))\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "input_ids = torch.tensor(padded_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0JTfw0B6MJZ",
        "outputId": "5a0291a2-8c78-4ce7-c639-3f663089305c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y build-essential libffi-dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVbNmWfI6SiH",
        "outputId": "9ee097a0-5522-461c-f7ff-e3a5b045b160"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libffi-dev is already the newest version (3.3-4).\n",
            "build-essential is already the newest version (12.8ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Config\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model_config = GPT2Config.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2', config=model_config)\n",
        "\n",
        "# Convert the model to the desired device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Fine-tune the model\n",
        "optimizer = Adam(model.parameters(), lr=5e-5, weight_decay=0.001)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=25)  \n",
        "batch_size = 8  \n",
        "gradient_accumulation_steps = 8  \n",
        "max_sequence_length = 256  \n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(25):  \n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "    accumulated_batches = 0\n",
        "\n",
        "    print(f\"Starting epoch {epoch}...\")\n",
        "\n",
        "    for i in range(0, len(input_ids), batch_size):\n",
        "        batch_input_ids = input_ids[i:i+batch_size]\n",
        "\n",
        "        # Truncate or split the input to the maximum sequence length\n",
        "        batch_input_ids = [ids[:max_sequence_length] for ids in batch_input_ids]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Convert the batch_input_ids to tensor and move to device\n",
        "        batch_input_ids = pad_sequence([torch.tensor(ids) for ids in batch_input_ids], batch_first=True).to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_input_ids, labels=batch_input_ids)\n",
        "        loss = outputs.loss\n",
        "        loss = loss / gradient_accumulation_steps  # Scale the loss\n",
        "        loss.backward()\n",
        "\n",
        "        accumulated_batches += 1\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if accumulated_batches == gradient_accumulation_steps:\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            accumulated_batches = 0\n",
        "\n",
        "        total_batches += 1\n",
        "\n",
        "        if total_batches % 100 == 0:\n",
        "            average_loss = total_loss / total_batches\n",
        "            print(f'Epoch {epoch}, Batch {total_batches}, Average Loss {average_loss:.4f}')\n",
        "\n",
        "   \n",
        "    if accumulated_batches > 0:\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    average_loss = total_loss / total_batches\n",
        "    print(f'Epoch {epoch}, Average Loss {average_loss:.4f}')\n",
        "\n",
        "    # Clear cache to free up GPU memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"Finished epoch {epoch}.\")\n",
        "\n",
        "# Save the further fine-tuned model\n",
        "model.save_pretrained('/gpt2-shakespeare')\n",
        "tokenizer.save_pretrained('./gpt2-shakespeare')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh-JhwKT6_oI",
        "outputId": "b2be1865-e9b2-450e-d057-bafebf90c174"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-53be050b5d91>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_input_ids = pad_sequence([torch.tensor(ids) for ids in batch_input_ids], batch_first=True).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average Loss 0.5378\n",
            "Finished epoch 0.\n",
            "Starting epoch 1...\n",
            "Epoch 1, Average Loss 0.5106\n",
            "Finished epoch 1.\n",
            "Starting epoch 2...\n",
            "Epoch 2, Average Loss 0.5054\n",
            "Finished epoch 2.\n",
            "Starting epoch 3...\n",
            "Epoch 3, Average Loss 0.5028\n",
            "Finished epoch 3.\n",
            "Starting epoch 4...\n",
            "Epoch 4, Average Loss 0.5025\n",
            "Finished epoch 4.\n",
            "Starting epoch 5...\n",
            "Epoch 5, Average Loss 0.5022\n",
            "Finished epoch 5.\n",
            "Starting epoch 6...\n",
            "Epoch 6, Average Loss 0.5010\n",
            "Finished epoch 6.\n",
            "Starting epoch 7...\n",
            "Epoch 7, Average Loss 0.4986\n",
            "Finished epoch 7.\n",
            "Starting epoch 8...\n",
            "Epoch 8, Average Loss 0.4951\n",
            "Finished epoch 8.\n",
            "Starting epoch 9...\n",
            "Epoch 9, Average Loss 0.4940\n",
            "Finished epoch 9.\n",
            "Starting epoch 10...\n",
            "Epoch 10, Average Loss 0.4929\n",
            "Finished epoch 10.\n",
            "Starting epoch 11...\n",
            "Epoch 11, Average Loss 0.4919\n",
            "Finished epoch 11.\n",
            "Starting epoch 12...\n",
            "Epoch 12, Average Loss 0.4921\n",
            "Finished epoch 12.\n",
            "Starting epoch 13...\n",
            "Epoch 13, Average Loss 0.4921\n",
            "Finished epoch 13.\n",
            "Starting epoch 14...\n",
            "Epoch 14, Average Loss 0.4918\n",
            "Finished epoch 14.\n",
            "Starting epoch 15...\n",
            "Epoch 15, Average Loss 0.4915\n",
            "Finished epoch 15.\n",
            "Starting epoch 16...\n",
            "Epoch 16, Average Loss 0.4908\n",
            "Finished epoch 16.\n",
            "Starting epoch 17...\n",
            "Epoch 17, Average Loss 0.4905\n",
            "Finished epoch 17.\n",
            "Starting epoch 18...\n",
            "Epoch 18, Average Loss 0.4901\n",
            "Finished epoch 18.\n",
            "Starting epoch 19...\n",
            "Epoch 19, Average Loss 0.4901\n",
            "Finished epoch 19.\n",
            "Starting epoch 20...\n",
            "Epoch 20, Average Loss 0.4898\n",
            "Finished epoch 20.\n",
            "Starting epoch 21...\n",
            "Epoch 21, Average Loss 0.4902\n",
            "Finished epoch 21.\n",
            "Starting epoch 22...\n",
            "Epoch 22, Average Loss 0.4900\n",
            "Finished epoch 22.\n",
            "Starting epoch 23...\n",
            "Epoch 23, Average Loss 0.4893\n",
            "Finished epoch 23.\n",
            "Starting epoch 24...\n",
            "Epoch 24, Average Loss 0.4900\n",
            "Finished epoch 24.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt2-shakespeare12/tokenizer_config.json',\n",
              " './gpt2-shakespeare12/special_tokens_map.json',\n",
              " './gpt2-shakespeare12/vocab.json',\n",
              " './gpt2-shakespeare12/merges.txt',\n",
              " './gpt2-shakespeare12/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE OR NOT TO BE(TEST!!!!!!)\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "\n",
        "model_path = './gpt2-shakespeare'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "\n",
        "prompt = \"juliet\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
        "\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-kZ6jC9ACmP",
        "outputId": "0757118b-c878-4105-978c-5853e50cc816"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "julét, and the king's son, and the queen's daughter, and the queen's daughter, and the queen's daughter, and the queen's daughter, and the queen's daughter, and the queen's daughter, and the queen's daughter, and the queen's daughter, and the queen's daughter, and the queen's daughter, and the queen's daughter, and the queen's daughter, and the queen's daughter, and the queen's daughter, and the queen's daughter,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKcsgLwNDlp3",
        "outputId": "0c696f83-66d0-4c21-c379-e334defc92b6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "source_path = '/content/gpt2-shakespeare12'  # Replace with the actual path of the zip file\n",
        "destination_path = '/content/drive/MyDrive/my_directory12.zip'  # Replace with the desired destination path in your Google Drive\n",
        "\n",
        "shutil.move(source_path, destination_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_DvG7rF3Dzyv",
        "outputId": "fe3ccf56-2171-47d3-af71-c3fd574afe05"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/my_directory12.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}